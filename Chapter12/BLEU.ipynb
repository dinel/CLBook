{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0e1c00d",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Bilingual-Evaluation-Understudy-Score-(BLEU)\" data-toc-modified-id=\"Bilingual-Evaluation-Understudy-Score-(BLEU)-1\">Bilingual Evaluation Understudy Score (BLEU)</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.-Quick-introduction-to-BLEU\" data-toc-modified-id=\"1.-Quick-introduction-to-BLEU-1.1\">1. Quick introduction to BLEU</a></span></li><li><span><a href=\"#2.-Calculating-BLEU-score-using-NLTK\" data-toc-modified-id=\"2.-Calculating-BLEU-score-using-NLTK-1.2\">2. Calculating BLEU score using NLTK</a></span></li><li><span><a href=\"#3.-Calculating-BLEU-score-manually\" data-toc-modified-id=\"3.-Calculating-BLEU-score-manually-1.3\">3. Calculating BLEU score manually</a></span><ul class=\"toc-item\"><li><span><a href=\"#3.1-The-modified-precision\" data-toc-modified-id=\"3.1-The-modified-precision-1.3.1\">3.1 The modified precision</a></span></li><li><span><a href=\"#3.2-Calculate-modified-precision-for-your-sentence\" data-toc-modified-id=\"3.2-Calculate-modified-precision-for-your-sentence-1.3.2\">3.2 Calculate modified precision for your sentence</a></span></li><li><span><a href=\"#3.3-Brevity-penalty\" data-toc-modified-id=\"3.3-Brevity-penalty-1.3.3\">3.3 Brevity penalty</a></span></li></ul></li><li><span><a href=\"#4.-Limitations-of-the-BLEU-score\" data-toc-modified-id=\"4.-Limitations-of-the-BLEU-score-1.4\">4. Limitations of the BLEU score</a></span><ul class=\"toc-item\"><li><span><a href=\"#4.1-Capitalisation-and-tokenisation\" data-toc-modified-id=\"4.1-Capitalisation-and-tokenisation-1.4.1\">4.1 Capitalisation and tokenisation</a></span></li><li><span><a href=\"#4.2-Syntactic-functions\" data-toc-modified-id=\"4.2-Syntactic-functions-1.4.2\">4.2 Syntactic functions</a></span></li></ul></li></ul></li><li><span><a href=\"#5.-Conclusions\" data-toc-modified-id=\"5.-Conclusions-2\">5. Conclusions</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba1b087",
   "metadata": {},
   "source": [
    "# Bilingual Evaluation Understudy Score (BLEU)\n",
    "\n",
    "In Section 12.5.2 of the book we presented the BLEU score. As we explained there, this is the most used automatic evaluation metric in machine translation. Despite being proposed in 2002, it is still employed by researchers who develop machine translation systems. The method was introduced in:\n",
    "\n",
    "> Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu (2002) BLEU: a Method for Automatic Evaluation of Machine Translation. In *Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL)*, Philadelphia, July 2002, pp. 311-318. https://www.aclweb.org/anthology/P02-1040/\n",
    "\n",
    "\n",
    "The purpose of this notebook is to discuss the BLUE score further and provide technical details which could not covered in the book due to space restrictions. It also provides snippets of code in Python for those interested to find out how to calculate the BLEU score for their examples. Some basic understanding of Python is required to follow the explanation below. \n",
    "\n",
    "We start with a short description of the BLEU score to highlight the main points about the evaluation metric. However, this discussion is not mean to replace the information presented in Chapter 12 related to evaluation in machine translation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6376679",
   "metadata": {},
   "source": [
    "## 1. Quick introduction to BLEU\n",
    "\n",
    "The aim of the BLEU score is to compare automatically the output of an MT engine to one or several human produced translations. The output of MT is referred to as *hypothesis*, whilst the human translations are referred to as *references*. As we already know, there is more than one correct translation for a given source sentence. For this reason, it is not possible to have a word by word comparison between the hypothesis and the references. Instead the BLEU score relies on a modified precision metric to calculate the overlap in terms of ngrams between the hypothesis and references.\n",
    "\n",
    "Usually, BLEU is calculated using ngrams of length from 1 to 4. This is referred to as BLEU-4. A weighting is assigned to each type of ngram, but usually an uniform distribution is used, where the weighting is 1/n. Given the way the score is calculated, it does not try to estimate the fluency or grammatical correctness of a translation directly. However, it was noted that unigram and bigram scores will largely capture the fidelity of the translation, whilst longer n-gram scores will account for fluency. \n",
    "\n",
    "The BLEU score is calculated using two parts:\n",
    "- the modified precision of ngrams from hypothesis in references, discussed in Section 3.1\n",
    "- brevity penalty (BP) which penalises translations which are significantly shorter than the references. This is discussed in Section 3.3.\n",
    "\n",
    "The BLEU score is calculated as geometric mean over the test corpusâ€™ modified precision scores multiplied by the brevity penalty factor. The formula used is:\n",
    "\n",
    "$BLEU = BP \\cdot \\exp (\\sum_{n=1}^{N}{w_n\\log p_n} )$\n",
    "\n",
    "In most of the cases, it is not necessary to know the formula to calculate the score because there are several software packages which can do that for you. We will start by showing how the BLEU score can be calculated using NLTK.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee199d20",
   "metadata": {},
   "source": [
    "## 2. Calculating BLEU score using NLTK\n",
    "\n",
    "To start, we will use the implementation of BLEU score from NLTK.\n",
    "\n",
    "We first import the necessary module and calculate the BLEU score using NLTK. This can be done without the need to understand precisely how the score is calculated. The relevant module is imported using ``from nltk.translate import bleu_score as bleu`` and more information about it can be found at https://www.nltk.org/api/nltk.translate.html#module-nltk.translate.bleu_score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7defb795",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate import bleu_score as bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40d5015",
   "metadata": {},
   "source": [
    "For our experiments, we will use the examples from (Papineni et. al, 2002), which are also discussed in the book:\n",
    "- Hypothesis (i.e. sentence produced by the MT engine): *the the the the the the the*\n",
    "- Reference A (i.e. translation produced by the first translator): *the cat is on the mat*\n",
    "- Reference B (i.e. translation produced by the second translator): *there is a cat on the mat*\n",
    "\n",
    "We initialise three variables which contain the tokens of the sentences as a list. For the sake of simplicity, in this case we assume that the space is the token separator, so we can use the string method ``split`` to produce a list of tokens for each sentences, which is the expected format of the methods we will use. Please note that we do not capitalise of the first word in the sentence. We do this for the sake of simplicity. This is particularly important and we will discuss this later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfb184ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp = 'the the the the the the the'.split()\n",
    "ref_a = 'the cat is on the mat'.split()\n",
    "ref_b = 'there is a cat on the mat'.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fb23aa",
   "metadata": {},
   "source": [
    "We will use the ``nltk.translate.bleu_score.sentence_bleu`` method to calculate the BLEU score at sentence level. This method takes a number of parameters, but we will discuss here only the first three ``references``, ``hypothesis``, and ``weights``. For the rest of the parameters the default values are a good starting point in most of the cases.\n",
    "- the ``references`` parameter is a list of lists which contains the reference translation(s). Each inner list contains the tokens of one of the reference translations. In the example where we use only the first reference, the value of this parameter is ``[ref_a]``. If we use both references the value for this parameter will be ``[ref_a, ref_b]``. Please note that even if we provide only one reference, we still need to have a list of lists even though the outer list contains only one element. \n",
    "- the ``hypothesis`` parameter is a list which contains the tokens of the hypothesis. In the examples below we use the ``hyp`` variable.\n",
    "- the ``weights`` parameter indicates the weights that is given to unigrams, bigrams and so on. The default behaviour of the implementation is to calculate the BLUE score for up to 4-grams using uniform weights (i.e. ``(0.25, 0.25, 0.25, 0.25)``). This BLEU score is sometimes called BLEU-4. If there is no match of ngrams for a given n (e.g. 3-grams and above), the BLEU score is zero. It is possible to use a smoothing function to avoid this, but we will not discuss it here. This smoothing function is one of the additional parameters which we do not discuss here. However, interested readers can find more details in the <a href=\"https://www.nltk.org/api/nltk.translate.html#module-nltk.translate.bleu_score\" target=\"_blank\">NLTK documentation</a>. If we want to calculate BLEU score up to 5-grams, we provide 5 values in the tuple. For uniform weights we would have ``(0.2, 0.2, 0.2, 0.2, 0.2)``. At the same time, if we want to use only unigrams, as in the examples discussed in the book, we can have a tuple with only one element (i.e. ``(1,)`` - keep in mind that the ``,`` is essential here because we have a tuple with only one element). For most of the discussion below, we will use unigrams. \n",
    "\n",
    "In the example below we calculate the BLEU score using each of the references individually and then together. In this particular case the best score is obtained when we use the first reference and there is no change of that score when we use both references. We will discuss this in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eefe6c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score for the hypothesis when using ref_a: 0.2857142857142857\n",
      "BLEU score for the hypothesis when using ref_b: 0.14285714285714285\n",
      "BLEU score for the hypothesis when using multiple refs: 0.2857142857142857\n"
     ]
    }
   ],
   "source": [
    "score_ref_a = bleu.sentence_bleu([ref_a], hyp, (1,) )\n",
    "print(f\"BLEU score for the hypothesis when using ref_a: {score_ref_a}\")\n",
    "score_ref_b = bleu.sentence_bleu([ref_b], hyp, (1,))\n",
    "print(f\"BLEU score for the hypothesis when using ref_b: {score_ref_b}\")\n",
    "score_ref_ab = bleu.sentence_bleu([ref_a, ref_b], hyp, (1,))\n",
    "print(f\"BLEU score for the hypothesis when using multiple refs: {score_ref_ab}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cc6acf",
   "metadata": {},
   "source": [
    "## 3. Calculating BLEU score manually\n",
    "\n",
    "We are now going to manually calculate the BLEU score manually, to explain how the values above were obtain. For this purpose we will use the code from NLTK to ensure the results are the same. \n",
    "\n"
   ]
  },
  {
   "attachments": {
    "e8f4ec18-6b62-4c91-ba46-d327721dc3ed.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAASMAAABHCAYAAABMMiHVAAAEQHRFWHRteGZpbGUAJTNDbXhmaWxl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\nIsAw0sktaiUBgwkwjAw2l6WRgE4E/gHxtSfC8MOEQAAAAABJRU5ErkJggg==\n"
    }
   },
   "cell_type": "markdown",
   "id": "1ff00bd6",
   "metadata": {},
   "source": [
    "### 3.1 The modified precision\n",
    "\n",
    "The BLUE score is a precision-based metric. This means that it checks how many of the ngrams from the hypothesis are present in the reference. However, we cannot use the normal precision because the metric would be easily fooled. For example, for the hypothesis we have above the normal precision is 1 because every unigram from the hypothesis appears in the reference(s). For this reason, the BLEU score uses a modified precision which counts the maximum number of times an ngram appears in each of the references. This maximum number of times is used to limit the number of matches allowed. For example for Reference 1 we can count only two *the*s, which means that the precision score is 2/7. Note that we divide by the number of ngrams in the hypothesis, not reference. The reference is used only to determine the maximum number of matches.\n",
    "\n",
    "![hyp-ref-a.dio.png](attachment:e8f4ec18-6b62-4c91-ba46-d327721dc3ed.png)\n",
    "\n",
    "Let's have a look how this is calculated for our examples. We will use some of the code from the NLTK implementation to explain. We first import the necessary modules and initialise the variable ``n`` to 1, to work with unigrams. We will need the ``nltk`` module to produce ngrams and the ``Counter`` datatype to produce frequency lists easily. (You can read more about how to produce frequency lists in Python at https://dinel.org.uk/2019/11/07/how-to-create-word-frequency-lists-in-python/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "117d997a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "\n",
    "n = 1 # change this number to work with bigrams (2), trigrams (3), and so on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e205ff",
   "metadata": {},
   "source": [
    "We produce all the ngrams using ``nltk.ngrams`` and count them using ``Counter``. In our case we have unigrams and the hypothesis contains only the word *the* 7 times. However, if the hypothesis was *The cat is on the mat on the floor* and we wanted to have the bigrams we would have\n",
    "\n",
    "```python\n",
    "counts = Counter(nltk.ngrams(\"The cat is on the mat on the floor\".split(), 2))\n",
    "print(counts)\n",
    "\n",
    ">> Counter({('on', 'the'): 2, ('The', 'cat'): 1, ('cat', 'is'): 1, ('is', 'on'): 1, ('the', 'mat'): 1, ('mat', 'on'): 1, ('the', 'floor'): 1})\n",
    "```\n",
    "\n",
    "We can see in the output that the bigram *on the* appears twice and all the other bigrams only once. \n",
    "\n",
    "If we go back to our example, we get the following lists of unigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42cf5c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({('the',): 7})\n"
     ]
    }
   ],
   "source": [
    "counts = Counter(nltk.ngrams(hyp, 1))\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9be11fd",
   "metadata": {},
   "source": [
    "We now produce the list of ngrams for the first reference as well. We see that the word *the* appears only twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "894257aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({('the',): 2, ('cat',): 1, ('is',): 1, ('on',): 1, ('mat',): 1})\n"
     ]
    }
   ],
   "source": [
    "reference_counts = Counter(nltk.ngrams(ref_a, n))\n",
    "print(reference_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef93a4e",
   "metadata": {},
   "source": [
    "As we can see in the output above, the only ngram that the reference and hypothesis have in common is *the*. Because *the* appears only twice in the reference our modified precision gives a score of 2/7.\n",
    "\n",
    "If we use the second reference, the result is lower because the maximum number of times we can count *the* in the second reference is 1. In this case, the score for modified precision is 1/7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ff985cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({('there',): 1, ('is',): 1, ('a',): 1, ('cat',): 1, ('on',): 1, ('the',): 1, ('mat',): 1})\n"
     ]
    }
   ],
   "source": [
    "reference_counts = Counter(nltk.ngrams(ref_b, n))\n",
    "print(reference_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5070e4c",
   "metadata": {},
   "source": [
    "### 3.2 Calculate modified precision for your sentence\n",
    "\n",
    "The following snippet of code adapted from the ``modified_precision`` implemented in NLTK (https://www.nltk.org/_modules/nltk/translate/bleu_score.html) gives you the opportunity to calculate the modified precision for your sentences. \n",
    "\n",
    "If you want to calculate the modified precision your sentences and using your parameters, you will need to change the following variables. As in the examples above, the tokenisation is done using space. You may want to use a better tokeniser like ``word_tokenize`` from NLTK (see more at <a href=\"https://www.nltk.org/_modules/nltk/tokenize.html#word_tokenize\" target=\"_blank\">https://www.nltk.org/_modules/nltk/tokenize.html#word_tokenize</a>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95f5e483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# edit the following strings to experiment with your sentences\n",
    "hyp = 'the the the the the the the'.split()\n",
    "ref_a = 'the cat is on the mat'.split()\n",
    "ref_b = 'there is a cat on the mat'.split()\n",
    "\n",
    "# if you want you can add more references using the format above, but make sure you add them to the list below\n",
    "references = [ref_a, ref_b]\n",
    "\n",
    "# change n if you want to use sequences of tokens longer than unigrams\n",
    "n = 1 \n",
    "\n",
    "# change the following variable if you use sequences of tokens longer than unigrams\n",
    "# for example for bigrams you will probably want to have (0.5, 0.5), for trigrams (1/3, 1/3, 1/3), for 4-grams (0.25, 0.25, 0.25, 0.25)\n",
    "weights = (1, )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dc5870",
   "metadata": {},
   "source": [
    "Once the variables are initialised, you will need to run the following snippet of code. If you want to do it outside this notebook, you can copy the code from the cell above and the one below in your favourite editor and run them as a script. You will need make sure you include the ``import``s from above. The code is a bit more complicated than what we had above because here we need to be able to process several references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b8a668a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The frequency list of the hyphothesis is  Counter({('the',): 7})\n",
      "The maximum number of times we can count the ngrams from the hypothesis is  {('the',): 2}\n",
      "The modified precision for is 0.2857142857142857\n"
     ]
    }
   ],
   "source": [
    "# Extracts all ngrams in hypothesis\n",
    "\n",
    "# Set an empty Counter if hypothesis is empty.\n",
    "counts = Counter(nltk.ngrams(hyp, n)) if len(hyp) >= n else Counter()\n",
    "print(\"The frequency list of the hyphothesis is \", counts)\n",
    "\n",
    "# process now the references\n",
    "max_counts = {}\n",
    "for reference in references:\n",
    "    reference_counts = Counter(nltk.ngrams(reference, n)) if len(reference) >= n else Counter()\n",
    "    for ngram in counts: max_counts[ngram] = max(max_counts.get(ngram, 0), reference_counts[ngram])\n",
    "\n",
    "print(\"The maximum number of times we can count the ngrams from the hypothesis is \", max_counts)\n",
    "\n",
    "# Assigns the intersection between hypothesis and references' counts.\n",
    "# This dictionary comprehension is necessary because an ngram may appear more times in a reference than in a hypothesis\n",
    "# In that case we will need to use only the number of times it appears in the hypothesis\n",
    "clipped_counts = {\n",
    "    ngram: min(count, max_counts[ngram]) for ngram, count in counts.items()\n",
    "}\n",
    "numerator = sum(clipped_counts.values())\n",
    "denominator = max(1, sum(counts.values()))\n",
    "print(\"The modified precision for is\", numerator/denominator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebdb84b",
   "metadata": {},
   "source": [
    "### 3.3 Brevity penalty\n",
    "\n",
    "The BLEU score penalises translations which are shorter than the reference. This is necessary because if the hypothesis contains only the word *the*, the modified precision score is very likely to be 1 because nearly any reference will contain the word *the* at least once. No penalty is introduced if the hypothesis is longer than the reference because it is assumed that the score is reduced enough as a result of the ngrams from the hypothesis which cannot be matched in the reference. \n",
    "\n",
    "The brevity penalty (BP) is calculated using $e^{1-\\frac{r}{h}}$, where ``r`` is the length in tokens of the reference and ``h`` is the length in tokens of the hypothesis. If there are several references available, the one with closest length is selected. \n",
    "\n",
    "In the example below, we show how the length of the hypothesis impacts on the brevity penalty and the BLEU score. We will use for reference the sentence *the cat is on the mat* and for hypothesis we will have an increasing number of <em>the</em>s. We start with one occurrence of the word *the* and increase the length of the hypothesis till the sentence contains the word *the* eight times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0df1340e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len hyp\t\tBrevity penalty\t\tModified precision\tBLEU score\n",
      "---------------------------------------------------------------------------\n",
      "   1\t\t    0.00674\t\t   1.00000\t\t   0.00674\n",
      "   2\t\t    0.13534\t\t   1.00000\t\t   0.13534\n",
      "   3\t\t    0.36788\t\t   0.66667\t\t   0.24525\n",
      "   4\t\t    0.60653\t\t   0.50000\t\t   0.30327\n",
      "   5\t\t    0.81873\t\t   0.40000\t\t   0.32749\n",
      "   6\t\t    1.00000\t\t   0.33333\t\t   0.33333\n",
      "   7\t\t    1.00000\t\t   0.28571\t\t   0.28571\n",
      "   8\t\t    1.00000\t\t   0.25000\t\t   0.25000\n"
     ]
    }
   ],
   "source": [
    "ref = 'the cat is on the mat'.split()\n",
    "\n",
    "import math\n",
    "\n",
    "print(\"Len hyp\\t\\tBrevity penalty\\t\\tModified precision\\tBLEU score\")\n",
    "print(\"-\" * 75)\n",
    "for i in [1, 2, 3, 4, 5, 6, 7, 8]:\n",
    "    hyp = ['the'] * i\n",
    "    score_ref_a = bleu.sentence_bleu([ref], hyp, (1,) )\n",
    "    modified_precision = float(bleu.modified_precision([ref], hyp, n=1))\n",
    "    brevity_penalty = math.exp(1 - len(ref) / len(hyp)) if len(hyp) < len(ref) else 1\n",
    "    print(f\"{i:4d}\\t\\t{brevity_penalty:11.5f}\\t\\t{modified_precision:10.5f}\\t\\t{score_ref_a:10.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d722d826",
   "metadata": {},
   "source": [
    "As we can see in the table above, the highest penalty is when the hypothesis contains only one word *the* and decreases as the length of the hypothesis increases. Remember that in order to obtain the BLUE score, we have to multiply the brevity penalty with the modified precision. Given that BP is always between 0 and 1, this means that a lower score for the brevity penalty translates into a bigger penalty of the BLEU score. Once the length of the hypothesis reaches the length of the reference (6 in this case), the brevity penalty is 1 and does not change. Because of the multiplication, this means no penalty is applied to the modified precision. \n",
    "\n",
    "In the table we also included the value for the modified precision and the final BLUE score. In order to keep the code short, we calculated these using the methods provided by NLTK. Interestingly enough, the highest score is obtained when the hypothesis has the correct length, despite not being informative at all. This shows the importance of brevity penalty.\n",
    "\n",
    "As we mentioned before, when the hypothesis is longer than the reference the value of brevity penalty is 1 (i.e. no penalty) because the modified precision reduces the BLEU score. This can be seen in the results above for the hypotheses with 7 and 8 tokens.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdd9aff",
   "metadata": {},
   "source": [
    "## 4. Limitations of the BLEU score\n",
    "\n",
    "The BLEU score is not only widely used, but also widely criticised. Most of the critism is directed towards the fact that the comparison between the hypothesis and references is made at token level. This means that a mismatch of a function word is treated exactly the same way as a difference in a content word. In addition, BLEU does not take into consideration the sentence structure which can lead to strange situations when there is a match in the ngrams, but they serve completely different syntactic roles. In this section, we show a few examples to make the reader aware of the limitations of BLEU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fd2735",
   "metadata": {},
   "source": [
    "### 4.1 Capitalisation and tokenisation\n",
    "\n",
    "Given that the comparison is done between ngrams, the way the input is preprocessed is very important. In section 2, we mention that we do not capitalise the first word in sentences. The way we deal with capitalisation is very important. In the example below the BLEU score is higher if we use ``ref_a`` instead of ``ref_b`` because with the first reference we can match two <em>the</em>s, whilst with the second there is only one lower case *the*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56f5549f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score when we have the token the twice: 0.2857142857142857\n",
      "Score when we have the token the once: 0.14285714285714285\n"
     ]
    }
   ],
   "source": [
    "ref_a = 'the cat is on the mat'.split()\n",
    "ref_b = 'The cat is on the mat'.split()\n",
    "hyp = 'the the the the the the the'.split()\n",
    "\n",
    "print(\"Score when we have the token the twice:\", bleu.sentence_bleu([ref_a], hyp, (1,) ))\n",
    "print(\"Score when we have the token the once:\", bleu.sentence_bleu([ref_b], hyp, (1,) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4f5a36",
   "metadata": {},
   "source": [
    "The way tokenisation is applied to a sentence also influences the results of the BLEU score. In the examples below we have five hypotheses (hyp_1-hyp_5), which say more or less the same as the reference. Hypotheses 1 and 2, say exactly the same thing as the reference, but using different contractions for *he is not*. Hypotheses 3 and 5 paraphrase the reference, and BLEU seems to correctly identify the case where the hypothesis is the closest to the reference (hypothesis 3). Scaringly enough, the highest score is obtained by hypothesis 6 which says exactly the opposite to the reference. In all these cases we use BLUE-4 which is the default for ``bleu.sentence_bleu`` if no weights are provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8994f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyp_1: 0.7888119293172784\n",
      "Hyp_2: 0.8696160820399262\n",
      "Hyp_3: 0.8656098697665126\n",
      "Hyp_4: 0.665126048082759\n",
      "Hyp_5: 0.5993999480037718\n",
      "Hyp_6: 0.8776467090813088\n"
     ]
    }
   ],
   "source": [
    "ref = \"He is not happy he is not going to cinema\"\n",
    "hyp_1 = \"He isn 't happy he isn 't going to cinema\"\n",
    "hyp_2 = \"He 's not happy he 's not going to cinema\"\n",
    "hyp_3 = \"He is unhappy he is not going to cinema\"\n",
    "hyp_4 = \"He 's not happy he 's not going to the movies\"\n",
    "hyp_5 = \"He is not happy he is staying home\"\n",
    "hyp_6 = \"He is happy he is not going to cinema\"\n",
    "\n",
    "print(\"Hyp_1:\", bleu.sentence_bleu([ref], hyp_1))\n",
    "print(\"Hyp_2:\", bleu.sentence_bleu([ref], hyp_2))\n",
    "print(\"Hyp_3:\", bleu.sentence_bleu([ref], hyp_3))\n",
    "print(\"Hyp_4:\", bleu.sentence_bleu([ref], hyp_4))\n",
    "print(\"Hyp_5:\", bleu.sentence_bleu([ref], hyp_5))\n",
    "print(\"Hyp_6:\", bleu.sentence_bleu([ref], hyp_6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a19c9df",
   "metadata": {},
   "source": [
    "Hypothesis 6 exemplifies a very serious problem that BLEU can cause in cases where changing a word in a sentence changes the meaning of this sentence completely. One of these cases is translation of sentiment as discussed in \n",
    "\n",
    "> Saadany, H., Orasan, C., Mohamed, E., and Tantawy, A. (2021) Sentiment-Aware Measure (SAM) for Evaluating Sentiment Transfer by Machine Translation Systems. In *Proceedings of Recent Advances in Natural Language Processing*, pages 1217â€“1226, Sep 1â€“3, 2021. https://arxiv.org/abs/2109.14895"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6a47a3",
   "metadata": {},
   "source": [
    "Different tokenisations and normalisations applied to references and hypotheses make it difficult to compare the results of different MT systems. For example representing the diacritics differently in the reference and hypothesis leads to a reduction of the BLEU score, and the removal of diacritics increases the score as discuss in https://twitter.com/prajdabre1/status/1422809646645972993:\n",
    "\n",
    "<blockquote class=\"twitter-tweet\" data-theme=\"dark\"><p lang=\"en\" dir=\"ltr\">Can anyone explain why English to Romanian translation is evaluated after removing diacritics in Romanian? Seems like an easy way to boost BLEU scores by 7 points :/ <a href=\"https://twitter.com/hashtag/NLProc?src=hash&amp;ref_src=twsrc%5Etfw\">#NLProc</a></p>&mdash; Raj Dabre (@prajdabre1) <a href=\"https://twitter.com/prajdabre1/status/1422809646645972993?ref_src=twsrc%5Etfw\">August 4, 2021</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n",
    "\n",
    "One solution to the problems introduced by different tokenisation and normalisation is proposed in\n",
    "\n",
    "> Post, M. (2018). A Call for Clarity in Reporting BLEU Scores. In *Proceedings of the Third Conference on Machine Translation: Research Papers*, 186â€“191. https://doi.org/10.18653/v1/W18-6319\n",
    "\n",
    "who argues that there should be more transparency when reporting the BLEU results and that the same tools should be used to calculate the BLEU score. He developed SacreBLEU (https://github.com/mjpost/sacrebleu) to address this problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a12227c",
   "metadata": {},
   "source": [
    "### 4.2 Syntactic functions\n",
    "\n",
    "In the example below the first hypothesis receives a higher score than the second even though it has a completely different meaning than the reference. In this case we use unigrams and bigram for scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4e1e7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyp_1: 0.8944271909999159\n",
      "Hyp_1: 0.48795003647426655\n"
     ]
    }
   ],
   "source": [
    "ref = \"the dog is chasing the cat\".split()\n",
    "hyp1 = \"the cat is chasing the dog\".split()\n",
    "hyp2 = \"the cat is chased by the dog\".split()\n",
    "\n",
    "print(\"Hyp_1:\", bleu.sentence_bleu([ref], hyp1, (0.5, 0.5) ))\n",
    "print(\"Hyp_1:\", bleu.sentence_bleu([ref], hyp2, (0.5, 0.5) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4faac34",
   "metadata": {},
   "source": [
    "This shows another limitation of the BLEU score which can be easily fouled by active voice-pasive voice difference. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba90d09a",
   "metadata": {},
   "source": [
    "# 5. Conclusions\n",
    "\n",
    "The BLUE score is still widely used in evaluation of machine translation. In this notebook we explained how it is calculated and which are its limitations.\n",
    "\n",
    "The discussion about the BLEU score was in the context of evaluation of machine translation. However, BLEU score is widely employed in a variety of tasks which include generation of text like image captioning and text simplification. BLEU has also inspired metrics like the ROUGE score which is commonly used in automatic text summarisation. In contrast to BLEU, which is a precision based metric, the ROUGE score is recall based. More information about the ROUGE score can be found in \n",
    "\n",
    "> Lin, Chin-Yew (2004) ROUGE: A Package for Automatic Evaluation of Summaries. In *Proceedings of the Workshop on Text Summarization Branches Out*, Barcelona, Spain, 25 - 26 July 2004, pp. 74â€“81. https://www.aclweb.org/anthology/W04-1013/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
